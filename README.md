# XGBoost
Twelve regression models, including gradient-boosting ensembles and deep neural  networks, are then trained under Bayesian hyperparameter optimization. XGBoost Regression  (XGBR) emerges as the top performer, reaching an RÂ² of 0.997 with exceptionally low  MAE/RMSE while maintaining high computational efficiency.
